{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMV0lFBZU7KJO4GMQQ9fqhV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexk2206/tds_capstone/blob/Domi-DEV/Overview_Domi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5: Generate model output\n",
        "(Dominik Schuster)\n",
        "> Referred notebook: model_choice_and_fine_tuning.ipynb\n",
        "\n",
        "What seems like an easy task, isn't that simple in reality.\n",
        "But how comes?\n",
        "There are several difficulties to face, but the predominant one is the fact, that there are different question types.\n",
        "Is it possible to answer them via one model?\n",
        "Or are more than one necessary?\n",
        "Especially the multiple and single choice questions are tough to handle, because the model has to choose an answer from predefined options.\n",
        "\n",
        "We thought about several ways to treat them:\n",
        "\n",
        "\n",
        "*   With Zero-Shot Classification -> The model classifies the input. In our case the classes would be the options\n",
        "*   With Qestion-Answering-models (QA-models), where the best answer(s) is/are tested on similarity on the options and chosen if applicable\n",
        "*   With Multiple Choice models (mc models), where the model directly finds the best option(s)\n",
        "\n",
        "The direct approach seemed to be very promising for us, as we would not need to postprocess the outcome of the model.\n",
        "Maybe this would be the same for the Zero-Shot Classification, but in our considerations we came to the conclusion it maybe would not.\n",
        "\n",
        "As the questions of type 'DATE' and 'NUMBER' (open-ended (oe) questions) are different to that and one just has to extract the answer out of the text without mapping to the options, we decided to take another model for these.\n",
        "We chose a QA-model, which is quite more handy than a mc model.\n",
        "Besides, we used a text-summarization pipeline to summarize the context of 'TEXT' questions, as it doesn't seem reasonable to extract an answer out of these questions.\n",
        "\n",
        "Before looking into the code, we want to remark, that we concentrated on handling one question a time in the beginning.\n",
        "This made the task way easier.\n",
        "Our goal was to generalize our solution to the task where one has to answer to different questions at the same time having one big context.\n",
        "Unfortunatly, we didn't come that far.\n"
      ],
      "metadata": {
        "id": "bOQ13ux_mwNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But now, let's jump into the code.\n",
        "Our main operator was the model_ouput() function, that is able to generate output for multiple questions (for which a intended answer is passed on) one after another, collecting all the answers and returning it in the end.\n",
        "Moreover, it can calculate metrics for the mc and the oe questions on the fly, where the expected output is compared to the real output.\n"
      ],
      "metadata": {
        "id": "QyHjVJuKBL-P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "291C7eg1mtna"
      },
      "outputs": [],
      "source": [
        "def model_output(mc_model, mc_tokenizer, oe_model, oe_tokenizer, questions, sum_pipeline=None, mc_metric=None, oe_metric=None):\n",
        "    '''\n",
        "    model_output -> creates output for every question in the dataset and safes it in a list of dicts\n",
        "    parameters:\n",
        "    - mc_model: one hugging face model for mc questions\n",
        "    - mc_tokenizer: hugging face tokenizer for mc questions\n",
        "    - oe_model: one hugging face model for oe questions\n",
        "    - oe_tokenizer: hugging face tokenizer for oe questions\n",
        "    - questions: QA-dataset as pd.DataFrame\n",
        "    - sum_pipeline: huggingface text-summarization pipeline to handle 'TEXT' questions\n",
        "    - mc_metric: metric for evaluating mc questions\n",
        "    - oe_metric: metric for evaluating oe questions\n",
        "    output:\n",
        "    - mc_answer_comparison: list of dicts with keys 'model', 'intended_answer_binary', 'predicted_answer_binary', 'intended_answer', 'predicted_answer', 'type', 'difficulty'\n",
        "    - answer_comparison: list of dicts with keys 'model', 'intended_answer', 'predicted_answer', 'type', 'difficulty'\n",
        "    '''\n",
        "    answer_comparison = []\n",
        "    mc_answer_comparison = []\n",
        "    mc_model_name = mc_model.config._name_or_path\n",
        "    oe_model_name = oe_model.config._name_or_path\n",
        "\n",
        "    for index, question in questions.iterrows():\n",
        "        context = question['context']\n",
        "        question_text = question['question']\n",
        "        options = question['options']\n",
        "        question_type = question['type']\n",
        "        difficulty = question['difficulty']\n",
        "\n",
        "        mc_question_type = question_type in [\"MULTI_SELECT\", \"SINGLE_SELECT\"]\n",
        "\n",
        "        if question_type == \"MULTI_SELECT\":\n",
        "          intended_answer, intended_answer_binary, predicted_answer_binary, predicted_answer = multi_select_model_output(mc_model, mc_tokenizer, question, mc_metric)\n",
        "        elif question_type == \"SINGLE_SELECT\":\n",
        "          intended_answer, intended_answer_binary, predicted_answer_binary, predicted_answer = single_select_model_output(mc_model, mc_tokenizer, question, mc_metric)\n",
        "        elif question_type == \"TEXT\":\n",
        "          intended_answer, predicted_answer = text_model_output(question, sum_pipeline)\n",
        "          continue\n",
        "        elif question_type == \"NUMBER\":\n",
        "          intended_answer, predicted_answer = number_model_output(oe_model, oe_tokenizer, question, oe_metric)\n",
        "        elif question_type == \"DATE\":\n",
        "          intended_answer, predicted_answer = date_model_output(oe_model, oe_tokenizer, question, oe_metric)\n",
        "        else:\n",
        "          continue\n",
        "        if predicted_answer != intended_answer:\n",
        "          print('======= Wrong answer =======')\n",
        "          print(f\"Question: {question_text}\")\n",
        "          print(f\"Context: {context}\")\n",
        "          print(f\"The intended answer was: {intended_answer}\")\n",
        "          print(f\"The predicted answer was: {predicted_answer}\")\n",
        "          if mc_question_type:\n",
        "            print(f\"The intended answer in BINARY was: {intended_answer_binary}\")\n",
        "            print(f\"The predicted answer in BINARY was: {predicted_answer_binary}\\n\")\n",
        "          else:\n",
        "            print(\"\")\n",
        "        if mc_question_type:\n",
        "          mc_answer_comparison.append({'model': mc_model_name, 'intended_answer_binary': intended_answer_binary, 'predicted_answer_binary': predicted_answer_binary, 'intended_answer': intended_answer, 'predicted_answer': predicted_answer, 'type': question_type, 'difficulty': difficulty})\n",
        "        else:\n",
        "          answer_comparison.append({'model': oe_model_name, 'intended_answer': intended_answer, 'predicted_answer': predicted_answer, 'type': question_type, 'difficulty': difficulty})\n",
        "\n",
        "    # Compute metrics, if they were passed as arguments\n",
        "    if mc_metric is not None:\n",
        "      try:\n",
        "        mc_metric_result = mc_metric.compute()\n",
        "      except:\n",
        "        mc_metric_result = None\n",
        "    else:\n",
        "      mc_metric_result = None\n",
        "    if oe_metric is not None:\n",
        "      try:\n",
        "        oe_metric_result = oe_metric.compute()\n",
        "      except:\n",
        "        oe_metric_result = None\n",
        "    else:\n",
        "      oe_metric_result = None\n",
        "    return mc_answer_comparison, answer_comparison, mc_metric_result, oe_metric_result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model output itself is created in the function calls of the functions <question_type>_model_output().\n",
        "Examplarary, we want show the one for 'MULTI_SELECT' questions here.\n",
        "The tokenization of the input is made in tokenize_function(), which we therefore also show underneath.\n",
        "In that function, we pass the context as often as there are options, saved as a list. We also pass a list as a question, whereby each entry of the question (of the QA-dataset) is linked to an option. In doing so, the model is able to return something like probabilities for the options to be implicitly chosen.\n",
        "\n",
        "In the 'MULTI_SELECT' questions we had a special case, where we couldn't choose just the best answer.\n",
        "To overcome the problem, we took the mean and the standard deviation of the \"probabilities\" (logits) into account.\n",
        "We predicted every option that had a higher \"probability\" as the mean + 40% of the standard deviation.\n",
        "In doing so, we captured confident predictions, accounted for variability and prevented over- and under-selection."
      ],
      "metadata": {
        "id": "ZkHLKMyWC0Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example, tokenizer):\n",
        "    '''\n",
        "    Converts the question with its context and the given options for multi-/single-select questions, into IDs the model later can make sense of. Distinguishes between multi-/single-select and the other question types\n",
        "    parameters:\n",
        "    - expample: question of the QA-dataset with all its entries (question, context, options, type are urgently necessary)\n",
        "    - tokenizer: tokenizer of the model\n",
        "    output:\n",
        "    - tokenized: tokenized input example\n",
        "    '''\n",
        "    if example[\"type\"] == \"SINGLE_SELECT\" or example[\"type\"] == \"MULTI_SELECT\":\n",
        "      number_of_options = len(example[\"options\"])\n",
        "      first_sentence = [[example[\"context\"]] * number_of_options]  # Repeat context for each option\n",
        "      second_sentence = [[example[\"question\"] + \" \" + option] for option in example[\"options\"]]  # Pair with each option\n",
        "      tokenized = tokenizer(\n",
        "          sum(first_sentence, []),\n",
        "          sum(second_sentence, []),\n",
        "          padding=\"longest\",\n",
        "          truncation=True\n",
        "      )\n",
        "      # Un-flatten\n",
        "      return {k: [v[i:i+number_of_options] for i in range(0, len(v), number_of_options)] for k, v in tokenized.items()}\n",
        "\n",
        "    elif example['type'] == 'NUMBER':\n",
        "      tokenized = tokenizer(\n",
        "          example['context'],\n",
        "          example['question'],\n",
        "          truncation=\"only_second\",\n",
        "          max_length=384,\n",
        "          padding=\"max_length\",\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "    else:\n",
        "      tokenized = tokenizer(\n",
        "          example['question'],\n",
        "          example['context'],\n",
        "          truncation=\"only_second\",\n",
        "          max_length=384,\n",
        "          padding=\"max_length\",\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "XZbnldpFDmjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_select_model_output(model, tokenizer, question, metric=None):\n",
        "    '''\n",
        "    Handles a question, its context and its options for a multi-select question\n",
        "    parameters:\n",
        "    - model: one MC hugging face model\n",
        "    - tokenizer: MC hugging face tokenizer\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    - metric: metric for evaluating the model output (optional)\n",
        "    output:\n",
        "    - intended_answer: the correct/intended answers as a list of strings\n",
        "    - intended_answer_binary: the correct/intended answers as a list of binary variables, where each entry is one, if option is chosen, 0 else\n",
        "    - predicted_answer_binary: the predicted answers as a list of binary variables, where each entry is one, if option is chosen, 0 else\n",
        "    - high_score_answers: the predicted answers as a list of strings\n",
        "    '''\n",
        "    intended_answer = question['intended_answer']\n",
        "    options = question['options']\n",
        "\n",
        "    # creating input ids by tokenizing the question\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    input_ids = {key: torch.tensor(array) for key, array in input_ids.items()}\n",
        "\n",
        "    # generating the output\n",
        "    outputs = model(**input_ids)\n",
        "    logits = outputs.logits  # Shape: [batch_size, num_choices]\n",
        "    print(logits)\n",
        "\n",
        "    ### Use a threshold from deviation and take all options that are higher than the mean + 40% of standard deviation\n",
        "    mean_score = logits.mean().item()\n",
        "    std_dev = logits.std().item()\n",
        "    threshold = mean_score + (0.4 * std_dev)\n",
        "    high_score_options = (logits >= threshold).nonzero(as_tuple=True)[1]  # Get the indices of valid options\n",
        "\n",
        "    # List the corresponding options\n",
        "    high_score_answers = [options[idx] for idx in high_score_options.tolist()]\n",
        "    intended_answer_binary = [1 if option in intended_answer else 0 for option in options]\n",
        "\n",
        "    predicted_answer_binary = [1 if option in high_score_answers else 0 for option in options]\n",
        "\n",
        "    # Add the results to the metric\n",
        "    if metric is not None:\n",
        "        metric.add_batch(predictions=predicted_answer_binary, references=intended_answer_binary)\n",
        "\n",
        "    return intended_answer, intended_answer_binary, predicted_answer_binary, high_score_answers"
      ],
      "metadata": {
        "id": "Oxu-xoMVDkSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For observing the other output functions, please look into model_choice_and_fine_tuning.ipynb.\n",
        "\n",
        "With that, we were able to produce some output and compare different models.\n",
        "That brought us to the task of choosing the model we wanted to fine-tune."
      ],
      "metadata": {
        "id": "S02twTbeF0cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 6: Model Choice\n",
        "(Dominik Schuster)\n",
        "\n",
        "As we wanted to take into account as much as we could and also to get a better feeling for the model output, we chose all of the metrics \"accuracy\", \"f1\", \"precision\" and \"recall\" for the mc model questions. For the oe model questions, only \"exact match\", as receiving a false phone number and/or date would be very problematic. The last task ('TEXT' questions) was not evaluated, since there is no real basis on which we can extract the right or wrong answer. The notes were only summarized.\n",
        "\n",
        "Since the model at this stage wouldn't remember the intended answers of the questions, we just used the training part of the QA-dataset. This didn't result in overfitting/underfitting a model and the dataset was big enough to really get a glimpse of how good the models perform.\n",
        "\n",
        "### Model for open-ended questions\n",
        "\n",
        "We took the \"distilbert-base-uncased-distilled-squad\" for oe questions, a QA model fine-tuned on the squad dataset.\n",
        "The metric results were so high, that we didn't see the advantage of trying out more models\n",
        "\n",
        "\n",
        "```\n",
        "The exact_match metric for all open-ended questions in the train dataset: 0.9651162790697675\n",
        "```\n",
        "\n",
        "### Model for multiple and single choice questions\n",
        "\n",
        "We decided to test a BERT, ALBERT, XLNet and RoBERTa model each. All of them can handle the same type of input and are able to weight the options. A weight what we afterwards use to predict the right option(s).\n",
        "\n",
        "The outcome:\n",
        "\n",
        "\n",
        "```\n",
        "bert-base-cased: {'accuracy': 0.8473297213622291, 'f1': 0.7606915377616015, 'precision': 0.7660354306658522, 'recall': 0.755421686746988}\n",
        "xlnet/xlnet-base-cased: {'accuracy': 0.7078977932636469, 'f1': 0.5054080629301868, 'precision': 0.5538793103448276, 'recall': 0.46473779385171793}\n",
        "FacebookAI/roberta-base: {'accuracy': 0.575687185443283, 'f1': 0.27989487516425754, 'precision': 0.3075812274368231, 'recall': 0.25678119349005424}\n",
        "albert/albert-base-v2: {'accuracy': 0.6314363143631436, 'f1': 0.3789954337899543, 'precision': 0.4129353233830846, 'recall': 0.350210970464135}\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "e8aQkB0dGcxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7: Fine-tuning\n",
        "(Dominik Schuster)\n",
        "\n",
        "The model \"bert-base-cased\" was doing quite well on the QA-dataset we created.\n",
        "But also \"albert/albert-base-v2\" managed get quite good results in view to the relatively little size of the model.\n",
        "Also it is known for faster learning.\n",
        "This is the reason why we wanted to fine-tune both models.\n",
        "Remark that we didn't do any fine-tuning on the oe models, as it was quite good instantly.\n",
        "\n",
        "Including all the code here would be very unhandy.\n",
        "To observe it, look into model_choice_and_fine_tuning.ipynb.\n",
        "But to give a glimpse, we post the fine_tune_model() function in here.\n",
        "We used the Hugging Face's Trainer API, which simplifies training & evaluation. Also we applied best practices for fine-tuning, including logging, evaluation, and model checkpointing.\n",
        "Also we defined our own DataCollator, the DataCollatorForMultipleChoice, to be able to handle the multiple choice questionsAfterwards, we save the fine-tuned model so it can be reused without retraining.\n"
      ],
      "metadata": {
        "id": "iUKKq05yIoMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(dataset, tokenizer, model, epochs, output_dir):\n",
        "    # Preprocess the dataset\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(output_dir=output_dir,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        learning_rate=4e-5,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset['train'],\n",
        "        eval_dataset=tokenized_dataset['test'],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "    save_path = f\"/content/drive/MyDrive/mc_models/{output_dir}\"\n",
        "    drive.mount('/content/drive')\n",
        "    # Create the directory if it does not exist\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "        print(f\"Directory created: {save_path}\")\n",
        "    else:\n",
        "        print(\"Directory already exists!\")\n",
        "    trainer.save_model(save_path)"
      ],
      "metadata": {
        "id": "rCPG_ewHKHum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter X: Conclusions\n",
        "Created by: Alexander Keßler & Dominik Schuster\n",
        "\n",
        "blablablablabalbalbalbalalbalbal\n",
        "\n",
        "## Limitations\n",
        "blablabalbalbabla\n",
        "\n",
        "## Further Improvements\n",
        "\n",
        "But despite facing the limitations mentioned above, we could still improve our code and by that our results.\n",
        "Additionally, the task we are now fulfilling with the model lacks in empirical realism, since we don't see an additional value in executing a survey and asking question by question.\n",
        "It would be just as easy as filling out the survey as one would have done before.\n",
        "\n",
        "So yes, there are some improvements to make.\n",
        "With the following list, we want to share our thoughts on that:\n",
        "\n",
        "\n",
        "1.   \n",
        "2.   In the context the\n",
        "\n"
      ],
      "metadata": {
        "id": "Llaivc1bx9wd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UTKi5nE62X9j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}