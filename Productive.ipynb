{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsLkPr9f8m/HfxjzCMl4Dn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexk2206/tds_capstone/blob/Domi-DEV/Productive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Productive Notebook"
      ],
      "metadata": {
        "id": "0mkPc0ZfHFc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SLjPgWr_1-kF",
        "outputId": "6ab8b133-e8e4-4c92-cda9-4cf592a3e877"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting sympy\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.13.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "b13d1f35be424706891936f2cf5f553c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import numpy as np\n",
        "import urllib\n",
        "from itertools import chain, combinations\n",
        "from transformers import AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments, pipeline, Trainer, DataCollatorWithPadding\n",
        "import torch\n",
        "import requests\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from typing import Optional, Union\n",
        "\n"
      ],
      "metadata": {
        "id": "9KoU8tBBI45u"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess dataset\n",
        "\n",
        "Here we split the QA-dataset into train and validation dataset.\n",
        "Additionnaly, we prepare the dataset to later be useful for response-generation and fine-tuning of a model"
      ],
      "metadata": {
        "id": "7zDNcgGRHOQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "❎ Please insert code: load dataset into variable q ❎"
      ],
      "metadata": {
        "id": "V7BIx4AbJJou"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AXL4FbBrHERD"
      },
      "outputs": [],
      "source": [
        "# Example dataset\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/alexk2206/tds_capstone/refs/heads/main/datasets/sampled_qa_dataset_easy.json\"\n",
        "data = pd.read_json(url)\n",
        "# Convert to DataFrame for easy handling\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Map the intended answer to the index of the option\n",
        "df['label'] = df.apply(lambda x: np.array([1 if option in x['intended_answer'] else 0 for option in x['options']]) if x['type'] in ['SINGLE_SELECT', 'MULTI_SELECT'] else np.array([0]), axis=1)\n",
        "df['stratify_key'] = df['difficulty'] + '_' + df['type']\n",
        "\n",
        "# Stratified Train-Validation Split\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    train_size=0.8,\n",
        "    stratify=df['stratify_key'],\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4mjdSg-PlNJ",
        "outputId": "bb65598a-d4fa-404b-cef6-03b9086b24c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             question           type  \\\n",
            "0                         What type of company is it?  SINGLE_SELECT   \n",
            "1   Would you like to receive marketing informatio...  SINGLE_SELECT   \n",
            "2                   What is the size of your company?  SINGLE_SELECT   \n",
            "3                                          Next steps  SINGLE_SELECT   \n",
            "4                   What kind of follow up is planned   MULTI_SELECT   \n",
            "..                                                ...            ...   \n",
            "95                  What is the size of your company?  SINGLE_SELECT   \n",
            "96                            Data processing consent  SINGLE_SELECT   \n",
            "97                        What type of company is it?  SINGLE_SELECT   \n",
            "98                           Who to copy in follow up   MULTI_SELECT   \n",
            "99                            Data processing consent  SINGLE_SELECT   \n",
            "\n",
            "                                              options  \\\n",
            "0   [Construction company, Craft enterprises, Scaf...   \n",
            "1                                           [Yes, No]   \n",
            "2   [1-10, 11-50, 51-200, 201-2000, larger than 2000]   \n",
            "3                              [Offer, Meeting, Call]   \n",
            "4         [Email, Phone, Schedule a Visit, No action]   \n",
            "..                                                ...   \n",
            "95  [1-10, 11-50, 51-200, 201-2000, larger than 2000]   \n",
            "96                                          [Yes, No]   \n",
            "97  [Construction company, Craft enterprises, Scaf...   \n",
            "98  [Stephan Maier, Joachim Wagner, Erik Schneider...   \n",
            "99                                          [Yes, No]   \n",
            "\n",
            "                                  intended_answer  \\\n",
            "0                          [Construction company]   \n",
            "1                                           [Yes]   \n",
            "2                                          [1-10]   \n",
            "3                                       [Meeting]   \n",
            "4     [Email, Phone, Schedule a Visit, No action]   \n",
            "..                                            ...   \n",
            "95                             [larger than 2000]   \n",
            "96                                          [Yes]   \n",
            "97                            [Craft enterprises]   \n",
            "98  [Joachim Wagner, Jessica Hanke, Domiki Stein]   \n",
            "99                                          [Yes]   \n",
            "\n",
            "                                              context difficulty  \\\n",
            "0   It's a construction company; that is, they bui...       easy   \n",
            "1   Yes, I'd like to receive marketing emails; tha...       easy   \n",
            "2   We're a small company; I'd say we're in the 1-...       easy   \n",
            "3   Next, I need to schedule a meeting to discuss ...       easy   \n",
            "4   I might email you, call you on the phone, sche...       easy   \n",
            "..                                                ...        ...   \n",
            "95  I work for a pretty big company; it's larger t...       easy   \n",
            "96         Yes, I consent to my data being processed.       easy   \n",
            "97  It's a craft enterprises company, meaning they...       easy   \n",
            "98  I should probably copy Joachim Wagner, Jessica...       easy   \n",
            "99         Yes, I consent to my data being processed.       easy   \n",
            "\n",
            "                                      label        stratify_key  \n",
            "0                        [1, 0, 0, 0, 0, 0]  easy_SINGLE_SELECT  \n",
            "1                                    [1, 0]  easy_SINGLE_SELECT  \n",
            "2                           [1, 0, 0, 0, 0]  easy_SINGLE_SELECT  \n",
            "3                                 [0, 1, 0]  easy_SINGLE_SELECT  \n",
            "4                              [1, 1, 1, 1]   easy_MULTI_SELECT  \n",
            "..                                      ...                 ...  \n",
            "95                          [0, 0, 0, 0, 1]  easy_SINGLE_SELECT  \n",
            "96                                   [1, 0]  easy_SINGLE_SELECT  \n",
            "97                       [0, 1, 0, 0, 0, 0]  easy_SINGLE_SELECT  \n",
            "98  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]   easy_MULTI_SELECT  \n",
            "99                                   [1, 0]  easy_SINGLE_SELECT  \n",
            "\n",
            "[100 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate model output\n",
        "\n",
        "After the creation of the QA-dataset, it's time for generating model output for different Huggingface models."
      ],
      "metadata": {
        "id": "8iquDfG7gYdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_output(model, tokenizer, questions):\n",
        "    '''\n",
        "    model_output -> creates output for every question in the dataset and safes it in a list of dicts. One dic has keys 'answer', 'predicted_answer', 'type'\n",
        "    parameters:\n",
        "    - model: one hugging face model\n",
        "    - tokenizer: hugging face tokenizer\n",
        "    - questions: QA-dataset in json format\n",
        "    '''\n",
        "    answer_comparison = []\n",
        "    mc_answer_comparison = []\n",
        "\n",
        "    for index, question in questions.iterrows():\n",
        "        context = question['context']\n",
        "        question_text = question['question']\n",
        "        options = question['options']\n",
        "        question_type = question['type']\n",
        "        difficulty = question['difficulty']\n",
        "\n",
        "        if question_type == \"MULTI_SELECT\":\n",
        "          intended_answer, intended_answer_binary, predicted_answer_binary, predicted_answer = multi_select_model_output(model, tokenizer, question)\n",
        "        if question_type == \"SINGLE_SELECT\":\n",
        "          intended_answer, intended_answer_binary, predicted_answer_binary, predicted_answer_words = single_select_model_output(model, tokenizer, question)\n",
        "        if question_type == \"TEXT\":\n",
        "          predicted_answer = text_model_output(question)\n",
        "        if question_type == \"NUMBER\":\n",
        "          predicted_answer = number_model_output(model, tokenizer, question)\n",
        "        if question_type == \"DATE\":\n",
        "          predicted_answer = date_model_output(model, tokenizer, question)\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "        print(f\"Question: {question_text}\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Answer: {intended_answer}\")\n",
        "        print(f\"Predicted Answer: {predicted_answer}\\n\")\n",
        "        if question_type in [\"MULTI_SELECT\", \"SINGLE_SELECT\"]:\n",
        "          mc_answer_comparison.append({'intended_answer_binary': intended_answer_binary, 'predicted_answer_binary': predicted_answer_binary, 'intended_answer': intended_answer, 'predicted_answer': predicted_answer, 'type': question_type, 'difficulty': difficulty})\n",
        "        else:\n",
        "          answer_comparison.append({'intended_answer': intended_answer, 'predicted_answer': predicted_answer, 'type': question_type, 'difficulty': difficulty})\n",
        "    return mc_answer_comparison, answer_comparison\n"
      ],
      "metadata": {
        "id": "tWqdWvQrgtle"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-select output"
      ],
      "metadata": {
        "id": "MwvDlSsVG3XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_select_model_output(model, tokenizer, question):\n",
        "    '''\n",
        "    Handles a question, its context and its options for a single-select question and generates output\n",
        "    parameters:\n",
        "    - model: one hugging face model\n",
        "    - tokenizer: hugging face tokenizer\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    output:\n",
        "    - answer: the correct/intended answer as a list of a string\n",
        "    - predicted_answer: the predicted answer as a list of a string\n",
        "    '''\n",
        "    intended_answer = question['intended_answer']\n",
        "    options = question['options']\n",
        "\n",
        "    # creating input ids by tokenizing the question\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    input_ids = {key: torch.tensor(array) for key, array in input_ids.items()}\n",
        "\n",
        "    # generating the output\n",
        "    outputs = model(**input_ids)\n",
        "    logits = outputs.logits  # Shape: [batch_size, num_choices]\n",
        "\n",
        "    # Predict the option with the highest score\n",
        "    predicted_option = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    predicted_binary = [0] * len(options)\n",
        "    predicted_binary[predicted_option] = 1\n",
        "\n",
        "    intended_binary = [1 if option == intended_answer else 0 for option in options]\n",
        "\n",
        "    return intended_answer, intended_answer_binary, predicted_answer_binary, options[predicted_option]\n"
      ],
      "metadata": {
        "id": "KgWVBNWlGkrC"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-select output"
      ],
      "metadata": {
        "id": "4XwAHnUIGy-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_select_model_output(model, tokenizer, question):\n",
        "    '''\n",
        "    Handles a question, its context and its options for a multi-select question and generates output as a list of indices of the predicted answers. Ticks every option whose probability is at least 90% of the best option (softmax)\n",
        "    parameters:\n",
        "    - model: one hugging face model\n",
        "    - tokenizer: hugging face tokenizer\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    output:\n",
        "    - answer: the correct/intended answers as a list of strings\n",
        "    - predicted_answer: the predicted answers as a list of strings\n",
        "    '''\n",
        "    intended_answer = question['intended_answer']\n",
        "    options = question['options']\n",
        "\n",
        "    # creating input ids by tokenizing the question\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    input_ids = {key: torch.tensor(array) for key, array in input_ids.items()}\n",
        "\n",
        "    # generating the output\n",
        "    outputs = model(**input_ids)\n",
        "    logits = outputs.logits  # Shape: [batch_size, num_choices]\n",
        "\n",
        "    # Find all indices to have at least 70% of the max score\n",
        "    max_score = logits.max().item()\n",
        "    threshold = 0.7 * max_score\n",
        "    high_score_options = (logits >= threshold).nonzero(as_tuple=True)[1]  # Get the indices of valid options\n",
        "\n",
        "    # List the corresponding options\n",
        "    high_score_answers = [options[idx] for idx in high_score_options.tolist()]\n",
        "    intended_answer_binary = [1 if option == intended_answer else 0 for option in options]\n",
        "\n",
        "    predicted_answer_binary = [1 if option in high_score_answers else 0 for option in options]\n",
        "    print(intended_answer)\n",
        "    print(high_score_answers)\n",
        "\n",
        "    return intended_answer, intended_answer_binary, predicted_answer_binary, high_score_answers\n",
        "\n"
      ],
      "metadata": {
        "id": "V52Kz-lNGqca"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text output"
      ],
      "metadata": {
        "id": "UXc_BEivG7nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_model_output(question):\n",
        "    '''\n",
        "    Handles an open text question and summarizes it\n",
        "    parameter:\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    output:\n",
        "    - answer: the full context of the question as a string\n",
        "    - summary: the generated summary as a string\n",
        "    '''\n",
        "    intended_answer = question['context']\n",
        "    summarization_pipeline = pipeline(\"text-summarization\")\n",
        "    summary = summarization_pipeline(intended_answer, max_length=100, min_length=30, do_sample=False)\n",
        "    return intended_answer, summary[0]['summary_text']\n",
        "\n"
      ],
      "metadata": {
        "id": "D7dedyjuGwXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phone Number output"
      ],
      "metadata": {
        "id": "XhVsQSw7G-G2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def number_model_output(model, tokenizer, question):\n",
        "    '''\n",
        "    Handles a question where the context should contain a phone number and generates an answer to that question\n",
        "    '''\n",
        "    intended_answer = question['intended_answer']\n",
        "\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    output = model(**input_ids)\n",
        "    predicted_number = output.logits.item()\n",
        "\n",
        "    return intended_answer, predicted_number\n",
        "\n"
      ],
      "metadata": {
        "id": "7h0RxjaPGvXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date output"
      ],
      "metadata": {
        "id": "9UZp9MuGHCoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def date_model_output(model, tokenizer, question):\n",
        "    '''\n",
        "    Handles a question where the context should contain a date and generates an answer to that question\n",
        "    '''\n",
        "    intended_answer = question['intended_answer']\n",
        "\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    output = model(**input_ids)\n",
        "    predicted_date = output.logits.item()\n",
        "\n",
        "    return intended_answer, predicted_date\n",
        "\n"
      ],
      "metadata": {
        "id": "0trBt0BSGuQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy"
      ],
      "metadata": {
        "id": "qBzA_tYRHFDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(answer_comparison):\n",
        "    '''\n",
        "    Computes the total accuracy and accuracy for each question type for the passed list of dicts. One dict in the list is one question with keys 'answer', 'predicted_answer', 'type'\n",
        "    parameters:\n",
        "    - list of dicts with entries 1) predicted answer 2) answer 3) type of question\n",
        "    '''\n",
        "    correct_multi_select = 0\n",
        "    correct_single_select = 0\n",
        "    correct_text = 0\n",
        "    correct_number = 0\n",
        "    correct_date = 0\n",
        "    correct_total = 0\n",
        "    total = 0\n",
        "\n",
        "    for entry in answer_comparison:\n",
        "        question_type = entry['type']\n",
        "        if entry['intended_answer'] == entry['predicted_answer']:\n",
        "            if question_type == 'MULTI_SELECT':\n",
        "                correct_multi_select += 1\n",
        "                total_multi_select += 1\n",
        "            elif question_type == 'SINGLE_SELECT':\n",
        "                correct_single_select += 1\n",
        "                total_single_select += 1\n",
        "            elif question_type == 'TEXT':\n",
        "                correct_text += 1\n",
        "                total_text += 1\n",
        "            elif question_type == 'NUMBER':\n",
        "                correct_number += 1\n",
        "                total_number += 1\n",
        "            elif question_type == 'DATE':\n",
        "                correct_date += 1\n",
        "                total_date += 1\n",
        "            else:\n",
        "              continue\n",
        "            correct_total += 1\n",
        "        total += 1\n",
        "    accuracy_total = correct_total / total\n",
        "    accuracy_multi_select = correct_multi_select / total_multi_select\n",
        "    accuracy_single_select = correct_single_select / total_single_select\n",
        "    accuracy_text = correct_text / total_text\n",
        "    accuracy_number = correct_number / total_number\n",
        "    accuracy_date = correct_date / total_date\n",
        "    return accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date\n",
        "'''\n",
        "print_out_model_quality: takes the computations of function accuracy() and prints them out\n",
        "parameters:\n",
        "- accuracy_total\n",
        "- accuracy_multi_select\n",
        "- accuracy_single_select\n",
        "- accuracy_text\n",
        "- accuracy_number\n",
        "- accuracy_date\n",
        "'''\n",
        "def print_out_model_quality(accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date):\n",
        "    # accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date = accuracy(model, tokenizer, questions)\n",
        "    print(f\"\"\"Accuracy values of model: {model.name_or_path}\\n\n",
        "    Total: {accuracy_total}\\n\n",
        "    Multi-select: {accuracy_multi_select}\\n\n",
        "    Single-select: {accuracy_single_select}\\n\n",
        "    Text: {accuracy_text}\\n\n",
        "    Number: {accuracy_number}\\n\n",
        "    Date: {accuracy_date}\\n\"\"\")\n",
        "    return accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fZ1JBWSeGsP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning a model\n"
      ],
      "metadata": {
        "id": "O_wI4yRfO835"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features):\n",
        "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
        "        labels = [feature.pop(label_name) for feature in features]\n",
        "        batch_size = len(features)\n",
        "        num_choices = len(features[0][\"input_ids\"])\n",
        "        flattened_features = [\n",
        "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
        "        ]\n",
        "        flattened_features = sum(flattened_features, [])\n",
        "\n",
        "        batch = self.tokenizer.pad(\n",
        "            flattened_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
        "        return batch"
      ],
      "metadata": {
        "id": "4VeNF--QgYRF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(train_dataset, val_dataset, tokenizer, model):\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\"trainer\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        learning_rate=2e-5,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
        "\n",
        "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "def compute_metrics(eval_preds, pretrained_dataset_name):\n",
        "    metric = evaluate.load(\"glue\", pretrained_dataset_name)\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "def tokenize_function(example, tokenizer):\n",
        "    '''\n",
        "    Converts the string input, which is a question with its context and the given options for multi-/single-select questions, into IDs the model later can make sense of. Distinguishes between multi-/single-select and the other questions\n",
        "    parameters:\n",
        "    - expample: question of the QA-dataset with all its entries (question, context, options, type are urgently necessary)\n",
        "    - tokenizer: tokenizer of the model\n",
        "    output:\n",
        "    - tokenized: tokenized input example\n",
        "    '''\n",
        "    if example[\"type\"] == \"SINGLE_SELECT\" or example[\"type\"] == \"MULTI_SELECT\":\n",
        "      number_of_options = len(example[\"options\"])\n",
        "      first_sentence = [[example[\"context\"]] * number_of_options]  # Repeat context for each option\n",
        "      second_sentence = [[example[\"question\"] + \" \" + option] for option in example[\"options\"]]  # Pair with each option\n",
        "      tokenized = tokenizer(\n",
        "          sum(first_sentence, []),\n",
        "          sum(second_sentence, []),\n",
        "          padding=\"longest\",\n",
        "          truncation=True\n",
        "      )\n",
        "      # Un-flatten\n",
        "      return {k: [v[i:i+number_of_options] for i in range(0, len(v), number_of_options)] for k, v in tokenized.items()}\n",
        "\n",
        "    else:\n",
        "      tokenized = tokenizer(\n",
        "          example[\"context\"],\n",
        "          example[\"question\"],\n",
        "          truncation=True,\n",
        "          max_length=512,\n",
        "          padding=\"max_length\",\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "      tokenized[\"labels\"] = 0\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "U_k1UsB9O-l4"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForMultipleChoice.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "q95bFIxx1X8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb695b69-d9f8-49c9-891f-4477135e1bb0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n"
      ],
      "metadata": {
        "id": "_nfqze4CXwXF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tryout = train_df[train_df['type'] == 'MULTI_SELECT']\n",
        "tryout = tryout.tail(1)"
      ],
      "metadata": {
        "id": "b_3e5ddjkTPd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tryout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "3PoyoBzWkwTQ",
        "outputId": "9369c907-467d-482f-fdba-27489565143d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   question          type  \\\n",
              "76  Searches a solution for  MULTI_SELECT   \n",
              "\n",
              "                                              options  \\\n",
              "76  [Scan business cards, Clean up CRM, Extract da...   \n",
              "\n",
              "                                      intended_answer  \\\n",
              "76  [Extract data from emails, Improve CRM data qu...   \n",
              "\n",
              "                                              context difficulty  \\\n",
              "76  I'm trying to find a way to extract data from ...       easy   \n",
              "\n",
              "              label       stratify_key  \n",
              "76  [0, 0, 1, 1, 1]  easy_MULTI_SELECT  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d387fd3-f5dd-4d30-afbd-c53e0a4f74fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>type</th>\n",
              "      <th>options</th>\n",
              "      <th>intended_answer</th>\n",
              "      <th>context</th>\n",
              "      <th>difficulty</th>\n",
              "      <th>label</th>\n",
              "      <th>stratify_key</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>Searches a solution for</td>\n",
              "      <td>MULTI_SELECT</td>\n",
              "      <td>[Scan business cards, Clean up CRM, Extract da...</td>\n",
              "      <td>[Extract data from emails, Improve CRM data qu...</td>\n",
              "      <td>I'm trying to find a way to extract data from ...</td>\n",
              "      <td>easy</td>\n",
              "      <td>[0, 0, 1, 1, 1]</td>\n",
              "      <td>easy_MULTI_SELECT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d387fd3-f5dd-4d30-afbd-c53e0a4f74fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8d387fd3-f5dd-4d30-afbd-c53e0a4f74fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8d387fd3-f5dd-4d30-afbd-c53e0a4f74fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_6b5957dc-c45d-4481-997e-1c4b9cf9623a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tryout')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6b5957dc-c45d-4481-997e-1c4b9cf9623a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('tryout');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tryout",
              "summary": "{\n  \"name\": \"tryout\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Searches a solution for\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"MULTI_SELECT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"options\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intended_answer\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"I'm trying to find a way to extract data from emails, improve the quality of my CRM data, and capture contacts I collect at trade fairs.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"difficulty\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"easy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stratify_key\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"easy_MULTI_SELECT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, question in tryout.iterrows():\n",
        "  tokenized = tokenize_function(question, tokenizer)\n",
        "  question_text = question['question']\n",
        "  options = question['options']\n",
        "  intended_answer = question['intended_answer']\n",
        "  context = question['context']"
      ],
      "metadata": {
        "id": "J5152UDbk62Y"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.decode(tokenized[\"input_ids\"][0][i]) for i in range(len(question[\"options\"]))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC8fJGJCobbt",
        "outputId": "02c9c529-017a-4dcd-bef3-afc0f6d95982"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"[CLS] I ' m trying to find a way to extract data from emails, improve the quality of my CRM data, and capture contacts I collect at trade fairs. [SEP] Searches a solution for Scan business cards [SEP] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I ' m trying to find a way to extract data from emails, improve the quality of my CRM data, and capture contacts I collect at trade fairs. [SEP] Searches a solution for Clean up CRM [SEP] [PAD] [PAD] [PAD]\",\n",
              " \"[CLS] I ' m trying to find a way to extract data from emails, improve the quality of my CRM data, and capture contacts I collect at trade fairs. [SEP] Searches a solution for Extract data from emails [SEP] [PAD] [PAD]\",\n",
              " \"[CLS] I ' m trying to find a way to extract data from emails, improve the quality of my CRM data, and capture contacts I collect at trade fairs. [SEP] Searches a solution for Improve CRM data quality [SEP]\",\n",
              " \"[CLS] I ' m trying to find a way to extract data from emails, improve the quality of my CRM data, and capture contacts I collect at trade fairs. [SEP] Searches a solution for Capture trade fair contacts [SEP] [PAD] [PAD]\"]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_1 = {key: torch.tensor(array) for key, array in tokenized.items()}\n",
        "# Pass to the model\n",
        "outputs = model(**tokenized_1).logits"
      ],
      "metadata": {
        "id": "ujMZ8ql6uLsA"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(outputs.shape)\n",
        "answer = torch.nn.functional.softmax(outputs, dim=-1)[0].argmax().item()\n",
        "\n",
        "print('======= Test Case 1 =======')\n",
        "print(f\"Question: {question}\\n\")\n",
        "print(f\"The answer is choice {answer}: {options[answer]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMVSlRB7wcoe",
        "outputId": "860e8ff5-b421-452b-e6a6-6984df205a1c"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Test Case 1 =======\n",
            "Question: question                                     Searches a solution for\n",
            "type                                                    MULTI_SELECT\n",
            "options            [Scan business cards, Clean up CRM, Extract da...\n",
            "intended_answer    [Extract data from emails, Improve CRM data qu...\n",
            "context            I'm trying to find a way to extract data from ...\n",
            "difficulty                                                      easy\n",
            "label                                                [0, 0, 1, 1, 1]\n",
            "stratify_key                                       easy_MULTI_SELECT\n",
            "Name: 76, dtype: object\n",
            "\n",
            "The answer is choice 4: Capture trade fair contacts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(question_text)\n",
        "print(context)\n",
        "mc_results, results = model_output(model, tokenizer, tryout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqaRbvYaVQ-l",
        "outputId": "a34d34c8-d806-45fc-9f8b-5d13d8ce9c28"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searches a solution for\n",
            "I'm trying to find a way to extract data from emails, improve the quality of my CRM data, and capture contacts I collect at trade fairs.\n",
            "['Extract data from emails', 'Improve CRM data quality', 'Capture trade fair contacts']\n",
            "['Clean up CRM', 'Improve CRM data quality', 'Capture trade fair contacts']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mc_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiAPAdEyV09h",
        "outputId": "f45876d8-9094-464a-c41a-f89e1dc79101"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AwK34z3JFMic"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}