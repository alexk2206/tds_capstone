{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpSEKkTQ/rQap+DpRdZo9T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexk2206/tds_capstone/blob/Domi-DEV/Productive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Productive Notebook"
      ],
      "metadata": {
        "id": "0mkPc0ZfHFc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLjPgWr_1-kF",
        "outputId": "8812a142-c050-4011-980e-10aef4c5be2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting sympy\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu121 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import numpy as np\n",
        "import urllib\n",
        "from itertools import chain, combinations\n",
        "from transformers import AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments\n",
        "import torch\n",
        "import requests\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "9KoU8tBBI45u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess dataset\n",
        "\n",
        "Here we split the QA-dataset into train and validation dataset.\n",
        "Additionnaly, we prepare the dataset to later be useful for response-generation and fine-tuning of a model"
      ],
      "metadata": {
        "id": "7zDNcgGRHOQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "❎ Please insert code: load dataset into variable q ❎"
      ],
      "metadata": {
        "id": "V7BIx4AbJJou"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AXL4FbBrHERD",
        "outputId": "74f9d859-82fa-4257-a671-de1eefd1c9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             question           type  \\\n",
            "0                         What type of company is it?  SINGLE_SELECT   \n",
            "1   Would you like to receive marketing informatio...  SINGLE_SELECT   \n",
            "2                   What is the size of your company?  SINGLE_SELECT   \n",
            "3                                          Next steps  SINGLE_SELECT   \n",
            "4                   What kind of follow up is planned   MULTI_SELECT   \n",
            "..                                                ...            ...   \n",
            "95                  What is the size of your company?  SINGLE_SELECT   \n",
            "96                            Data processing consent  SINGLE_SELECT   \n",
            "97                        What type of company is it?  SINGLE_SELECT   \n",
            "98                           Who to copy in follow up   MULTI_SELECT   \n",
            "99                            Data processing consent  SINGLE_SELECT   \n",
            "\n",
            "                                              options  \\\n",
            "0   [Construction company, Craft enterprises, Scaf...   \n",
            "1                                           [Yes, No]   \n",
            "2   [1-10, 11-50, 51-200, 201-2000, larger than 2000]   \n",
            "3                              [Offer, Meeting, Call]   \n",
            "4         [Email, Phone, Schedule a Visit, No action]   \n",
            "..                                                ...   \n",
            "95  [1-10, 11-50, 51-200, 201-2000, larger than 2000]   \n",
            "96                                          [Yes, No]   \n",
            "97  [Construction company, Craft enterprises, Scaf...   \n",
            "98  [Stephan Maier, Joachim Wagner, Erik Schneider...   \n",
            "99                                          [Yes, No]   \n",
            "\n",
            "                                  intended_answer  \\\n",
            "0                          [Construction company]   \n",
            "1                                           [Yes]   \n",
            "2                                          [1-10]   \n",
            "3                                       [Meeting]   \n",
            "4     [Email, Phone, Schedule a Visit, No action]   \n",
            "..                                            ...   \n",
            "95                             [larger than 2000]   \n",
            "96                                          [Yes]   \n",
            "97                            [Craft enterprises]   \n",
            "98  [Joachim Wagner, Jessica Hanke, Domiki Stein]   \n",
            "99                                          [Yes]   \n",
            "\n",
            "                                              context difficulty  \n",
            "0   It's a construction company; that is, they bui...       easy  \n",
            "1   Yes, I'd like to receive marketing emails; tha...       easy  \n",
            "2   We're a small company; I'd say we're in the 1-...       easy  \n",
            "3   Next, I need to schedule a meeting to discuss ...       easy  \n",
            "4   I might email you, call you on the phone, sche...       easy  \n",
            "..                                                ...        ...  \n",
            "95  I work for a pretty big company; it's larger t...       easy  \n",
            "96         Yes, I consent to my data being processed.       easy  \n",
            "97  It's a craft enterprises company, meaning they...       easy  \n",
            "98  I should probably copy Joachim Wagner, Jessica...       easy  \n",
            "99         Yes, I consent to my data being processed.       easy  \n",
            "\n",
            "[100 rows x 6 columns]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "['Construction company'] is not in list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c17fa149db15>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Map the intended answer to the index of the option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'options'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intended_answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stratify_key'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10372\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10373\u001b[0m         )\n\u001b[0;32m> 10374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10376\u001b[0m     def map(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_numba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c17fa149db15>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Map the intended answer to the index of the option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'options'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intended_answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stratify_key'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ['Construction company'] is not in list"
          ]
        }
      ],
      "source": [
        "# Example dataset\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/alexk2206/tds_capstone/refs/heads/main/datasets/sampled_qa_dataset_easy.json\"\n",
        "data = pd.read_json(url)\n",
        "print(data)\n",
        "# Convert to DataFrame for easy handling\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Map the intended answer to the index of the option\n",
        "df['label'] = df.apply(lambda x: x['options'].index(x['intended_answer']), axis=1)\n",
        "df['stratify_key'] = df['context_type'] + '_' + df['type']\n",
        "\n",
        "# Stratified Train-Validation Split\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    train_size=0.8,\n",
        "    stratify=df['stratify_key'],\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate model output\n",
        "\n",
        "After the creation of the QA-dataset, it's time for generating model output for different Huggingface models."
      ],
      "metadata": {
        "id": "8iquDfG7gYdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_output(model, tokenizer, questions):\n",
        "  '''\n",
        "  model_output -> creates output for every question in the dataset and safes it in a list of dicts. One dic has keys 'answer', 'predicted_answer', 'type'\n",
        "  parameters:\n",
        "  - model: one hugging face model\n",
        "  - tokenizer: hugging face tokenizer\n",
        "  - questions: QA-dataset in json format\n",
        "  '''\n",
        "  answer_comparison = []\n",
        "  for question in questions:\n",
        "        context = question['context']\n",
        "        question_text = question['question']\n",
        "        options = question['options']\n",
        "        answer = question['answer']\n",
        "        question_type = question['type']\n",
        "\n",
        "        if question_type == \"MULTI_SELECT\":\n",
        "          answer, predicted_answer = multi_select_model_output(model, tokenizer, question)\n",
        "        if question_type == \"SINGLE_SELECT\":\n",
        "          answer, predicted_answer = single_select_model_output(model, tokenizer, question)\n",
        "        if question_type == \"TEXT\":\n",
        "          answer, predicted_answer = text_model_output(question)\n",
        "        if question_type == \"NUMBER\":\n",
        "          answer, predicted_answer = number_model_output(model, tokenizer, question)\n",
        "        if question_type == \"DATE\":\n",
        "          answer, predicted_answer = date_model_output(model, tokenizer, question)\n",
        "        else:\n",
        "          continue\n",
        "        answer_comparison.append({'answer': answer, 'predicted_answer': options[predicted_option], 'type': question_type})\n",
        "  return answer_comparison\n",
        "\n",
        "def single_select_model_output(model, tokenizer, question):\n",
        "    '''\n",
        "    Handles a question, its context and its options for a single-select question and generates output\n",
        "    parameters:\n",
        "    - model: one hugging face model\n",
        "    - tokenizer: hugging face tokenizer\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    output:\n",
        "    - answer: the correct/intended answer as a list of a string\n",
        "    - predicted_answer: the predicted answer as a list of a string\n",
        "    '''\n",
        "    answer = question['answer']\n",
        "    options = question['options']\n",
        "\n",
        "    # creating input ids by tokenizing the question\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    input_ids = input_ids[\"input_ids\"].reshape(1, len(options), -1)\n",
        "    attention_mask = input_ids[\"attention_mask\"].reshape(1, len(options), -1)\n",
        "\n",
        "    # generating the output\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits  # Shape: [batch_size, num_choices]\n",
        "\n",
        "    # Predict the option with the highest score\n",
        "    predicted_option = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    predicted_binary = [0] * len(options)\n",
        "    predicted_binary[predicted_option] = 1\n",
        "\n",
        "    intended_binary = [0] * len(options)\n",
        "    intended_binary[option == answer for option in options] = 1\n",
        "\n",
        "    return intended_binary, predicted_binary, options[predicted_option]\n",
        "\n",
        "def multi_select_model_output(model, tokenizer, question):\n",
        "    '''\n",
        "    Handles a question, its context and its options for a multi-select question and generates output as a list of indices of the predicted answers. Ticks every option whose probability is at least 90% of the best option (softmax)\n",
        "    parameters:\n",
        "    - model: one hugging face model\n",
        "    - tokenizer: hugging face tokenizer\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    output:\n",
        "    - answer: the correct/intended answers as a list of strings\n",
        "    - predicted_answer: the predicted answers as a list of strings\n",
        "    '''\n",
        "    answer = question['answer']\n",
        "    options = question['options']\n",
        "\n",
        "    # creating input ids by tokenizing the question\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    input_ids = input_ids[\"input_ids\"].reshape(1, len(options), -1)\n",
        "    attention_mask = input_ids[\"attention_mask\"].reshape(1, len(options), -1)\n",
        "\n",
        "    # generating the output\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits  # Shape: [batch_size, num_choices]\n",
        "\n",
        "    # Find all indices to have at least 90% of the max score\n",
        "    max_score = logits.max().item()\n",
        "    threshold = 0.9 * max_score\n",
        "    high_score_options = (logits >= threshold).nonzero(as_tuple=True)[1]  # Get the indices of valid options\n",
        "\n",
        "    # List the corresponding options\n",
        "    high_score_answers = [options[idx] for idx in high_score_options.tolist()]\n",
        "    intended_binary = [0] * len(options)\n",
        "    intended_binary[option == answer for option in options] = 1\n",
        "\n",
        "    predicted_binary = [0] * len(options)\n",
        "    predicted_binary[high_score_options.tolist()] = 1\n",
        "\n",
        "    return intended_binary, predicted_binary, options[predicted_binary]\n",
        "\n",
        "def text_model_output(question):\n",
        "    '''\n",
        "    Handles an open text question and summarizes it\n",
        "    parameter:\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    output:\n",
        "    - answer: the full context of the question as a string\n",
        "    - summary: the generated summary as a string\n",
        "    '''\n",
        "    answer = question['context']\n",
        "    summarization_pipeline = pipeline(\"text-summarization\")\n",
        "    summary = summarization_pipeline(answer, max_length=100, min_length=30, do_sample=False)\n",
        "    return answer, summary[0]['summary_text']\n",
        "\n",
        "def number_model_output(model, tokenizer, question):\n",
        "    '''\n",
        "    Handles a question where the context should contain a phone number and generates an answer to that question\n",
        "    '''\n",
        "    answer = question['answer']\n",
        "\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    output = model(**input_ids)\n",
        "    predicted_number = output.logits.item()\n",
        "\n",
        "    return answer, predicted_number\n",
        "\n",
        "def date_model_output(model, tokenizer, question):\n",
        "    '''\n",
        "    Handles a question where the context should contain a date and generates an answer to that question\n",
        "    '''\n",
        "    answer = question['answer']\n",
        "\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    output = model(**input_ids)\n",
        "    predicted_date = output.logits.item()\n",
        "\n",
        "    return answer, predicted_date\n",
        "\n",
        "def accuracy(answer_comparison):\n",
        "    '''\n",
        "    Computes the total accuracy and accuracy for each question type for the passed list of dicts. One dict in the list is one question with keys 'answer', 'predicted_answer', 'type'\n",
        "    parameters:\n",
        "    - list of dicts with entries 1) predicted answer 2) answer 3) type of question\n",
        "    '''\n",
        "    correct_multi_select = 0\n",
        "    correct_single_select = 0\n",
        "    correct_text = 0\n",
        "    correct_number = 0\n",
        "    correct_date = 0\n",
        "    correct_total = 0\n",
        "    total = 0\n",
        "\n",
        "    for entry in answer_comparison:\n",
        "        question_type = entry['type']\n",
        "        if entry['answer'] == entry['predicted_answer']:\n",
        "            if question_type == 'MULTI_SELECT':\n",
        "                correct_multi_select += 1\n",
        "                total_multi_select += 1\n",
        "            elif question_type == 'SINGLE_SELECT':\n",
        "                correct_single_select += 1\n",
        "                total_single_select += 1\n",
        "            elif question_type == 'TEXT':\n",
        "                correct_text += 1\n",
        "                total_text += 1\n",
        "            elif question_type == 'NUMBER':\n",
        "                correct_number += 1\n",
        "                total_number += 1\n",
        "            elif question_type == 'DATE':\n",
        "                correct_date += 1\n",
        "                total_date += 1\n",
        "            else:\n",
        "              continue\n",
        "            correct_total += 1\n",
        "        total += 1\n",
        "    accuracy_total = correct_total / total\n",
        "    accuracy_multi_select = correct_multi_select / total_multi_select\n",
        "    accuracy_single_select = correct_single_select / total_single_select\n",
        "    accuracy_text = correct_text / total_text\n",
        "    accuracy_number = correct_number / total_number\n",
        "    accuracy_date = correct_date / total_date\n",
        "    return accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date\n",
        "'''\n",
        "print_out_model_quality: takes the computations of function accuracy() and prints them out\n",
        "parameters:\n",
        "- accuracy_total\n",
        "- accuracy_multi_select\n",
        "- accuracy_single_select\n",
        "- accuracy_text\n",
        "- accuracy_number\n",
        "- accuracy_date\n",
        "'''\n",
        "def print_out_model_quality(accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date):\n",
        "    accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date = accuracy(model, tokenizer, questions)\n",
        "    print(f\"\"\"Accuracy values of model: {model.name_or_path}\\n\n",
        "    Total: {accuracy_total}\\n\n",
        "    Multi-select: {accuracy_multi_select}\\n\n",
        "    Single-select: {accuracy_single_select}\\n\n",
        "    Text: {accuracy_text}\\n\n",
        "    Number: {accuracy_number}\\n\n",
        "    Date: {accuracy_date}\\n\"\"\")\n",
        "    return accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tWqdWvQrgtle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning a model\n"
      ],
      "metadata": {
        "id": "O_wI4yRfO835"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(train_dataset, val_dataset, tokenizer, model):\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\"trainer\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        learning_rate=2e-5,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "def compute_metrics(eval_preds, pretrained_dataset_name):\n",
        "    metric = evaluate.load(\"glue\", pretrained_dataset_name)\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "def tokenize_function(example, tokenizer):\n",
        "    '''\n",
        "    Converts the string input, which is a question with its context and the given options for multi-/single-select questions, into IDs the model later can make sense of. Distinguishes between multi-/single-select and the other questions\n",
        "    parameters:\n",
        "    - expample: question of the QA-dataset with all its entries (question, context, options, type are urgently necessary)\n",
        "    - tokenizer: tokenizer of the model\n",
        "    output:\n",
        "    - tokenized: tokenized input example\n",
        "    '''\n",
        "    if example[\"type\"] == \"SINGLE_SELECT\" or example[\"type\"] == \"MULTIPLE_SELECT\":\n",
        "      tokenized = tokenizer(\n",
        "          [example[\"context\"]] * len(example[\"options\"]),  # Repeat context for each option\n",
        "          [example[\"question\"] + \" \" + option for option in example[\"options\"]],  # Pair with each option\n",
        "          truncation=True,\n",
        "          max_length=512,\n",
        "          padding=\"max_length\",  # Ensure uniform input length\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "\n",
        "      # Get the label (index of the intended answer)\n",
        "      if example[\"type\"] == \"SINGLE_SELECT\":\n",
        "          # For single-select questions, just find the index of the correct option\n",
        "          labels = example[\"options\"].index(example[\"intended_answer\"])\n",
        "      elif example[\"type\"] == \"MULTIPLE_SELECT\":\n",
        "          # For multiple-select questions, create binary labels for each option\n",
        "          labels = [1 if option in example[\"intended_answer\"] else 0 for option in example[\"options\"]]\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown question type: {example['type']}\")\n",
        "      tokenized[\"labels\"] = labels  # Add labels to the tokenized output\n",
        "    else:\n",
        "      tokenized = tokenizer(\n",
        "          example[\"context\"],\n",
        "          example[\"question\"],\n",
        "          truncation=True,\n",
        "          max_length=512,\n",
        "          padding=\"max_length\",\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "      tokenized[\"labels\"] = 0\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "U_k1UsB9O-l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q95bFIxx1X8E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}