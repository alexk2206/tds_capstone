{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjlRJmG/Iz++eGmktnDwNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexk2206/tds_capstone/blob/Domi-DEV/Productive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Productive Notebook"
      ],
      "metadata": {
        "id": "0mkPc0ZfHFc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLjPgWr_1-kF",
        "outputId": "7f9708c3-4ce6-47ad-b5d2-b30f2fe681e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import numpy as np\n",
        "import urllib\n",
        "from itertools import chain, combinations\n",
        "from transformers import AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments, pipeline, Trainer, DataCollatorWithPadding\n",
        "import torch\n",
        "import requests\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from typing import Optional, Union\n",
        "\n"
      ],
      "metadata": {
        "id": "9KoU8tBBI45u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess dataset\n",
        "\n",
        "Here we split the QA-dataset into train and validation dataset.\n",
        "Additionnaly, we prepare the dataset to later be useful for response-generation and fine-tuning of a model"
      ],
      "metadata": {
        "id": "7zDNcgGRHOQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "❎ Please insert code: load dataset into variable q ❎"
      ],
      "metadata": {
        "id": "V7BIx4AbJJou"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "AXL4FbBrHERD"
      },
      "outputs": [],
      "source": [
        "# Example dataset\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/alexk2206/tds_capstone/refs/heads/main/datasets/sampled_qa_dataset_easy.json\"\n",
        "data = pd.read_json(url)\n",
        "# Convert to DataFrame for easy handling\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Map the intended answer to the index of the option\n",
        "df['label'] = df.apply(lambda x: np.array([1 if option in x['intended_answer'] else 0 for option in x['options']]) if x['type'] in ['SINGLE_SELECT', 'MULTI_SELECT'] else np.array([0]), axis=1)\n",
        "df['stratify_key'] = df['difficulty'] + '_' + df['type']\n",
        "\n",
        "# Stratified Train-Validation Split\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    train_size=0.8,\n",
        "    stratify=df['stratify_key'],\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate model output\n",
        "\n",
        "After the creation of the QA-dataset, it's time for generating model output for different Huggingface models."
      ],
      "metadata": {
        "id": "8iquDfG7gYdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_output(model, tokenizer, questions, mc_metric=None, other_metric=None):\n",
        "    '''\n",
        "    model_output -> creates output for every question in the dataset and safes it in a list of dicts. One dic has keys 'answer', 'predicted_answer', 'type'\n",
        "    parameters:\n",
        "    - model: one hugging face model\n",
        "    - tokenizer: hugging face tokenizer\n",
        "    - questions: QA-dataset in json format\n",
        "    '''\n",
        "    answer_comparison = []\n",
        "    mc_answer_comparison = []\n",
        "\n",
        "    for index, question in questions.iterrows():\n",
        "        context = question['context']\n",
        "        question_text = question['question']\n",
        "        options = question['options']\n",
        "        question_type = question['type']\n",
        "        difficulty = question['difficulty']\n",
        "\n",
        "        if question_type == \"MULTI_SELECT\":\n",
        "          intended_answer, intended_answer_binary, predicted_answer_binary, predicted_answer = multi_select_model_output(model, tokenizer, question, mc_metric)\n",
        "        elif question_type == \"SINGLE_SELECT\":\n",
        "          intended_answer, intended_answer_binary, predicted_answer_binary, predicted_answer_words = single_select_model_output(model, tokenizer, question, mc_metric)\n",
        "        elif question_type == \"TEXT\":\n",
        "          intended_answer, predicted_answer = text_model_output(question)\n",
        "        elif question_type == \"NUMBER\":\n",
        "          intended_answer, predicted_answer = number_model_output(model, tokenizer, question, other_metric)\n",
        "        elif question_type == \"DATE\":\n",
        "          intended_answer, predicted_answer = date_model_output(model, tokenizer, question, other_metric)\n",
        "        else:\n",
        "          continue\n",
        "        if predicted_answer != intended_answer:\n",
        "          print('======= Wrong answer =======')\n",
        "          print(f\"Question: {question}\")\n",
        "          print(f\"Context: {context}\")\n",
        "          print(f\"The intended answer was: {intended_answer}\")\n",
        "          print(f\"The predicted answer was: {predicted_answer}\\n\")\n",
        "        if question_type in [\"MULTI_SELECT\", \"SINGLE_SELECT\"]:\n",
        "          mc_answer_comparison.append({'intended_answer_binary': intended_answer_binary, 'predicted_answer_binary': predicted_answer_binary, 'intended_answer': intended_answer, 'predicted_answer': predicted_answer, 'type': question_type, 'difficulty': difficulty})\n",
        "        else:\n",
        "          answer_comparison.append({'intended_answer': intended_answer, 'predicted_answer': predicted_answer, 'type': question_type, 'difficulty': difficulty})\n",
        "    return mc_answer_comparison, answer_comparison\n"
      ],
      "metadata": {
        "id": "tWqdWvQrgtle"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-select output"
      ],
      "metadata": {
        "id": "MwvDlSsVG3XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_select_model_output(model, tokenizer, question, metric=None):\n",
        "    '''\n",
        "    Handles a question, its context and its options for a single-select question and generates output\n",
        "    parameters:\n",
        "    - model: one hugging face model\n",
        "    - tokenizer: hugging face tokenizer\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    output:\n",
        "    - answer: the correct/intended answer as a list of a string\n",
        "    - predicted_answer: the predicted answer as a list of a string\n",
        "    '''\n",
        "    intended_answer = question['intended_answer']\n",
        "    options = question['options']\n",
        "\n",
        "    # creating input ids by tokenizing the question\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    input_ids = {key: torch.tensor(array) for key, array in input_ids.items()}\n",
        "\n",
        "    # generating the output\n",
        "    outputs = model(**input_ids)\n",
        "    logits = outputs.logits  # Shape: [batch_size, num_choices]\n",
        "\n",
        "    # Predict the option with the highest score\n",
        "    predicted_option = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    predicted_answer_binary = [0] * len(options)\n",
        "    predicted_answer_binary[predicted_option] = 1\n",
        "\n",
        "    intended_answer_binary = [1 if option == intended_answer else 0 for option in options]\n",
        "\n",
        "    if metric is not None:\n",
        "      metric.add_batch(predictions=predicted_answer_binary, references=intended_answer_binary)\n",
        "\n",
        "    return intended_answer, intended_answer_binary, predicted_answer_binary, options[predicted_option]\n"
      ],
      "metadata": {
        "id": "KgWVBNWlGkrC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-select output"
      ],
      "metadata": {
        "id": "4XwAHnUIGy-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_select_model_output(model, tokenizer, question, metric=None):\n",
        "    '''\n",
        "    Handles a question, its context and its options for a multi-select question and generates output as a list of indices of the predicted answers. Ticks every option whose probability is at least 90% of the best option (softmax)\n",
        "    parameters:\n",
        "    - model: one hugging face model\n",
        "    - tokenizer: hugging face tokenizer\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    output:\n",
        "    - answer: the correct/intended answers as a list of strings\n",
        "    - predicted_answer: the predicted answers as a list of strings\n",
        "    '''\n",
        "    intended_answer = question['intended_answer']\n",
        "    options = question['options']\n",
        "\n",
        "    # creating input ids by tokenizing the question\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    input_ids = {key: torch.tensor(array) for key, array in input_ids.items()}\n",
        "\n",
        "    # generating the output\n",
        "    outputs = model(**input_ids)\n",
        "    logits = outputs.logits  # Shape: [batch_size, num_choices]\n",
        "\n",
        "    # Find all indices to have at least 70% of the max score\n",
        "    max_score = logits.max().item()\n",
        "    threshold = 0.70 * max_score\n",
        "    high_score_options = (logits >= threshold).nonzero(as_tuple=True)[1]  # Get the indices of valid options\n",
        "\n",
        "    # List the corresponding options\n",
        "    high_score_answers = [options[idx] for idx in high_score_options.tolist()]\n",
        "    intended_answer_binary = [1 if option == intended_answer else 0 for option in options]\n",
        "\n",
        "    predicted_answer_binary = [1 if option in high_score_answers else 0 for option in options]\n",
        "\n",
        "    if metric is not None:\n",
        "        metric.add_batch(predictions=predicted_answer_binary, references=intended_answer_binary)\n",
        "\n",
        "    return intended_answer, intended_answer_binary, predicted_answer_binary, high_score_answers\n",
        "\n"
      ],
      "metadata": {
        "id": "V52Kz-lNGqca"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text output"
      ],
      "metadata": {
        "id": "UXc_BEivG7nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_model_output(question):\n",
        "    '''\n",
        "    Handles an open text question and summarizes it\n",
        "    parameter:\n",
        "    - question: one question of the QA-dataset as a dictionary\n",
        "    output:\n",
        "    - answer: the full context of the question as a string\n",
        "    - summary: the generated summary as a string\n",
        "    '''\n",
        "    intended_answer = question['context']\n",
        "    summarization_pipeline = pipeline(\"text-summarization\")\n",
        "    summary = summarization_pipeline(intended_answer, max_length=100, min_length=30, do_sample=False)\n",
        "    return intended_answer, summary[0]['summary_text']\n",
        "\n"
      ],
      "metadata": {
        "id": "D7dedyjuGwXb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phone Number output"
      ],
      "metadata": {
        "id": "XhVsQSw7G-G2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def number_model_output(model, tokenizer, question, metric=None):\n",
        "    '''\n",
        "    Handles a question where the context should contain a phone number and generates an answer to that question\n",
        "    '''\n",
        "    intended_answer = question['intended_answer']\n",
        "\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    output = model(**input_ids)\n",
        "    start_logits, end_logits = output.start_logits, output.end_logits\n",
        "\n",
        "    # Get most probable start and end index\n",
        "    start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "    end_idx = torch.argmax(end_logits, dim=1).item() + 1  # Include last token\n",
        "\n",
        "    # Convert token IDs to text\n",
        "    predicted_tokens = tokenized[\"input_ids\"][0][start_idx:end_idx]\n",
        "    predicted_answer = tokenizer.decode(predicted_tokens, skip_special_tokens=True)\n",
        "\n",
        "    if metric is not None:\n",
        "        metric.add(predictions=predicted_number, references=intended_answer)\n",
        "\n",
        "    return intended_answer, predicted_number\n",
        "\n"
      ],
      "metadata": {
        "id": "7h0RxjaPGvXO"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date output"
      ],
      "metadata": {
        "id": "9UZp9MuGHCoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def date_model_output(model, tokenizer, question, metric=None):\n",
        "    '''\n",
        "    Handles a question where the context should contain a date and generates an answer to that question\n",
        "    '''\n",
        "    intended_answer = question['intended_answer']\n",
        "\n",
        "    input_ids = tokenize_function(question, tokenizer)\n",
        "    output = model(**input_ids)\n",
        "    predicted_date = output.logits.item()\n",
        "\n",
        "    if metric is not None:\n",
        "        metric.add(predictions=predicted_date, references=intended_answer)\n",
        "\n",
        "    return intended_answer, predicted_date\n",
        "\n"
      ],
      "metadata": {
        "id": "0trBt0BSGuQ4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy"
      ],
      "metadata": {
        "id": "qBzA_tYRHFDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(answer_comparison):\n",
        "    '''\n",
        "    Computes the total accuracy and accuracy for each question type for the passed list of dicts. One dict in the list is one question with keys 'answer', 'predicted_answer', 'type'\n",
        "    parameters:\n",
        "    - list of dicts with entries 1) predicted answer 2) answer 3) type of question\n",
        "    '''\n",
        "    correct_multi_select = 0\n",
        "    correct_single_select = 0\n",
        "    correct_text = 0\n",
        "    correct_number = 0\n",
        "    correct_date = 0\n",
        "    correct_total = 0\n",
        "    total = 0\n",
        "\n",
        "    for entry in answer_comparison:\n",
        "        question_type = entry['type']\n",
        "        if entry['intended_answer'] == entry['predicted_answer']:\n",
        "            if question_type == 'MULTI_SELECT':\n",
        "                correct_multi_select += 1\n",
        "                total_multi_select += 1\n",
        "            elif question_type == 'SINGLE_SELECT':\n",
        "                correct_single_select += 1\n",
        "                total_single_select += 1\n",
        "            elif question_type == 'TEXT':\n",
        "                correct_text += 1\n",
        "                total_text += 1\n",
        "            elif question_type == 'NUMBER':\n",
        "                correct_number += 1\n",
        "                total_number += 1\n",
        "            elif question_type == 'DATE':\n",
        "                correct_date += 1\n",
        "                total_date += 1\n",
        "            else:\n",
        "              continue\n",
        "            correct_total += 1\n",
        "        total += 1\n",
        "    accuracy_total = correct_total / total\n",
        "    accuracy_multi_select = correct_multi_select / total_multi_select\n",
        "    accuracy_single_select = correct_single_select / total_single_select\n",
        "    accuracy_text = correct_text / total_text\n",
        "    accuracy_number = correct_number / total_number\n",
        "    accuracy_date = correct_date / total_date\n",
        "    return accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date\n",
        "'''\n",
        "print_out_model_quality: takes the computations of function accuracy() and prints them out\n",
        "parameters:\n",
        "- accuracy_total\n",
        "- accuracy_multi_select\n",
        "- accuracy_single_select\n",
        "- accuracy_text\n",
        "- accuracy_number\n",
        "- accuracy_date\n",
        "'''\n",
        "def print_out_model_quality(accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date):\n",
        "    # accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date = accuracy(model, tokenizer, questions)\n",
        "    print(f\"\"\"Accuracy values of model: {model.name_or_path}\\n\n",
        "    Total: {accuracy_total}\\n\n",
        "    Multi-select: {accuracy_multi_select}\\n\n",
        "    Single-select: {accuracy_single_select}\\n\n",
        "    Text: {accuracy_text}\\n\n",
        "    Number: {accuracy_number}\\n\n",
        "    Date: {accuracy_date}\\n\"\"\")\n",
        "    return accuracy_total, accuracy_multi_select, accuracy_single_select, accuracy_text, accuracy_number, accuracy_date\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fZ1JBWSeGsP1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning a model\n"
      ],
      "metadata": {
        "id": "O_wI4yRfO835"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features):\n",
        "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
        "        labels = [feature.pop(label_name) for feature in features]\n",
        "        batch_size = len(features)\n",
        "        num_choices = len(features[0][\"input_ids\"])\n",
        "        flattened_features = [\n",
        "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
        "        ]\n",
        "        flattened_features = sum(flattened_features, [])\n",
        "\n",
        "        batch = self.tokenizer.pad(\n",
        "            flattened_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
        "        return batch"
      ],
      "metadata": {
        "id": "4VeNF--QgYRF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(train_dataset, val_dataset, tokenizer, model):\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\"trainer\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        learning_rate=2e-5,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "    data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
        "\n",
        "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "def compute_metrics(eval_preds, pretrained_dataset_name):\n",
        "    metric = evaluate.load(\"glue\", pretrained_dataset_name)\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "def tokenize_function(example, tokenizer):\n",
        "    '''\n",
        "    Converts the string input, which is a question with its context and the given options for multi-/single-select questions, into IDs the model later can make sense of. Distinguishes between multi-/single-select and the other questions\n",
        "    parameters:\n",
        "    - expample: question of the QA-dataset with all its entries (question, context, options, type are urgently necessary)\n",
        "    - tokenizer: tokenizer of the model\n",
        "    output:\n",
        "    - tokenized: tokenized input example\n",
        "    '''\n",
        "    if example[\"type\"] == \"SINGLE_SELECT\" or example[\"type\"] == \"MULTI_SELECT\":\n",
        "      number_of_options = len(example[\"options\"])\n",
        "      first_sentence = [[example[\"context\"]] * number_of_options]  # Repeat context for each option\n",
        "      second_sentence = [[example[\"question\"] + \" \" + option] for option in example[\"options\"]]  # Pair with each option\n",
        "      tokenized = tokenizer(\n",
        "          sum(first_sentence, []),\n",
        "          sum(second_sentence, []),\n",
        "          padding=\"longest\",\n",
        "          truncation=True\n",
        "      )\n",
        "      # Un-flatten\n",
        "      return {k: [v[i:i+number_of_options] for i in range(0, len(v), number_of_options)] for k, v in tokenized.items()}\n",
        "\n",
        "    else:\n",
        "      tokenized = tokenizer(\n",
        "          example[\"context\"],\n",
        "          example[\"question\"],\n",
        "          truncation=\"only_second\",\n",
        "          max_length=384,\n",
        "          padding=\"max_length\",\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "      for key in tokenized:\n",
        "        tokenized[key] = tokenized[key].unsqueeze(0)\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "U_k1UsB9O-l4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForMultipleChoice.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "q95bFIxx1X8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef990f93-0bf0-4a54-df70-a0496bf6b07f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n"
      ],
      "metadata": {
        "id": "_nfqze4CXwXF"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tryout = train_df[train_df['type'] == 'NUMBER']"
      ],
      "metadata": {
        "id": "b_3e5ddjkTPd"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tryout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "3PoyoBzWkwTQ",
        "outputId": "5ea28ad9-73d6-4ca7-d1f0-8063af4ad471"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     question    type         options  \\\n",
              "42  What phone number can we use for contact?  NUMBER  [phone number]   \n",
              "22  What phone number can we use for contact?  NUMBER  [phone number]   \n",
              "72  What phone number can we use for contact?  NUMBER  [phone number]   \n",
              "47  What phone number can we use for contact?  NUMBER  [phone number]   \n",
              "\n",
              "    intended_answer                                            context  \\\n",
              "42  [0165236859574]  My phone number is 0165236859574;  you can rea...   \n",
              "22   [011930396217]                  You can reach me at 011930396217.   \n",
              "72   [013591350004]  My phone number is 013591350004; you can reach...   \n",
              "47    [01884444852]                   You can reach me at 01884444852.   \n",
              "\n",
              "   difficulty label stratify_key  \n",
              "42       easy   [0]  easy_NUMBER  \n",
              "22       easy   [0]  easy_NUMBER  \n",
              "72       easy   [0]  easy_NUMBER  \n",
              "47       easy   [0]  easy_NUMBER  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae62b664-5538-4fc1-a2fb-0afef154ef11\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>type</th>\n",
              "      <th>options</th>\n",
              "      <th>intended_answer</th>\n",
              "      <th>context</th>\n",
              "      <th>difficulty</th>\n",
              "      <th>label</th>\n",
              "      <th>stratify_key</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>What phone number can we use for contact?</td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>[phone number]</td>\n",
              "      <td>[0165236859574]</td>\n",
              "      <td>My phone number is 0165236859574;  you can rea...</td>\n",
              "      <td>easy</td>\n",
              "      <td>[0]</td>\n",
              "      <td>easy_NUMBER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>What phone number can we use for contact?</td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>[phone number]</td>\n",
              "      <td>[011930396217]</td>\n",
              "      <td>You can reach me at 011930396217.</td>\n",
              "      <td>easy</td>\n",
              "      <td>[0]</td>\n",
              "      <td>easy_NUMBER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>What phone number can we use for contact?</td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>[phone number]</td>\n",
              "      <td>[013591350004]</td>\n",
              "      <td>My phone number is 013591350004; you can reach...</td>\n",
              "      <td>easy</td>\n",
              "      <td>[0]</td>\n",
              "      <td>easy_NUMBER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>What phone number can we use for contact?</td>\n",
              "      <td>NUMBER</td>\n",
              "      <td>[phone number]</td>\n",
              "      <td>[01884444852]</td>\n",
              "      <td>You can reach me at 01884444852.</td>\n",
              "      <td>easy</td>\n",
              "      <td>[0]</td>\n",
              "      <td>easy_NUMBER</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae62b664-5538-4fc1-a2fb-0afef154ef11')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae62b664-5538-4fc1-a2fb-0afef154ef11 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae62b664-5538-4fc1-a2fb-0afef154ef11');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6627ad9d-28f5-46c2-80f4-faaab45e3d58\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6627ad9d-28f5-46c2-80f4-faaab45e3d58')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6627ad9d-28f5-46c2-80f4-faaab45e3d58 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_278f76f4-df39-4c4b-952f-f351b411dac7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tryout')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_278f76f4-df39-4c4b-952f-f351b411dac7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('tryout');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tryout",
              "summary": "{\n  \"name\": \"tryout\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"What phone number can we use for contact?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NUMBER\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"options\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intended_answer\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"You can reach me at 011930396217.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"difficulty\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"easy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stratify_key\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"easy_NUMBER\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mc_results, results = model_output(model, tokenizer, tryout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "CqaRbvYaVQ-l",
        "outputId": "171b4162-27a4-43aa-f3f7-3fa5d0ac023d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MultipleChoiceModelOutput' object has no attribute 'start_logits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-fb53e26b0912>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmc_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtryout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-4c0ee512879d>\u001b[0m in \u001b[0;36mmodel_output\u001b[0;34m(model, tokenizer, questions, mc_metric, other_metric)\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mintended_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquestion_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NUMBER\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           \u001b[0mintended_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquestion_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"DATE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mintended_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-8aadebe8b3a2>\u001b[0m in \u001b[0;36mnumber_model_output\u001b[0;34m(model, tokenizer, question, metric)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Get most probable start and end index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MultipleChoiceModelOutput' object has no attribute 'start_logits'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiAPAdEyV09h",
        "outputId": "f09c4647-ac4c-4424-aa16-69c484344c66"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = AutoModelForMultipleChoice.from_pretrained(\"distilbert/distilbert-base-uncased-distilled-squad\")\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-distilled-squad\")\n",
        "mc_results, results = model_output(model2, tokenizer2, tryout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwK34z3JFMic",
        "outputId": "e99afd3d-38c1-4008-8950-245e7ce6f9de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForMultipleChoice were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-distilled-squad and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Extract data from emails', 'Improve CRM data quality', 'Capture trade fair contacts']\n",
            "['Scan business cards', 'Clean up CRM']\n",
            "Question: Searches a solution for\n",
            "Context: I'm trying to find a way to extract data from emails, improve the quality of my CRM data, and capture contacts I collect at trade fairs.\n",
            "Answer: ['Extract data from emails', 'Improve CRM data quality', 'Capture trade fair contacts']\n",
            "Predicted Answer: ['Scan business cards', 'Clean up CRM']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdbI8TJ8OgDF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}